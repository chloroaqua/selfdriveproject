{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "slim = tf.contrib.slim\n",
    "from tensorflow.python.util import nest\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "The presented model performs a mapping from sequences of images to sequences of steering angle measurements. The mapping is causal, i.e. there is no \"looking into future\" -- only past frames are used to predict the future steering decisions.  \n",
    "\n",
    "The model is based on three key components: 1) The input image sequences are processed with a 3D convolution stack, where the discrete time axis is interpreted as the first \"depth\" dimension. That allows the model to learn motion detectors and understand the dynamics of driving. 2) The model predicts not only the steering angle, but also the vehicle speed and the torque applied to the steering wheel. 3) The model is stateful: the two upper layers are a LSTM and a simple RNN, respectively. The predicted angle, torque and speed serve as the input to the next timestep.  \n",
    "\n",
    "The model is optimized jointly for the autoregressive and ground truth modes: in the former, model's own outputs are fed into next timestep, in the latter, real targets are used as the context. Naturally, only autoregressive mode is used at the test time.   \n",
    "\n",
    "I used a single GTX 1080 to train the model. In the training phase there was a constraint to fit into the memory of the card (8 GB). For the evaluation phase the model was performing nearly twice as fast as real-time in this setup.  \n",
    "\n",
    "Data extraction from rosbags is performed using Ross Wightman's scripts, because these were also used for the test data in this challenge; for real-life scenarios (and not for the challenge) it would make sense to read data directly into the model from the rosbags. Another concern about real-life is that the steering angle sequence that is to be predicted should be probably delayed by the actuator's latency.  \n",
    "\n",
    "No data augmentation (except for aggressive regularization via dropout) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some constants\n",
    "\n",
    "# RNNs are typically trained using (truncated) backprop through time. SEQ_LEN here is the length of BPTT. \n",
    "# Batch size specifies the number of sequence fragments used in a single optimization step.\n",
    "# (Actually we can use variable SEQ_LEN and BATCH_SIZE, they are set to constants only for simplicity).\n",
    "# LEFT_CONTEXT is the number of extra frames from the past that we append to the left of our input sequence.\n",
    "# We need to do it because 3D convolution with \"VALID\" padding \"eats\" frames from the left, \n",
    "# decreasing the sequence length.\n",
    "# One should be careful here to maintain the model's causality.\n",
    "SEQ_LEN = 10 \n",
    "BATCH_SIZE = 4 \n",
    "LEFT_CONTEXT = 5\n",
    "\n",
    "# These are the input image parameters.\n",
    "HEIGHT = 480\n",
    "WIDTH = 640\n",
    "CHANNELS = 3 # RGB\n",
    "\n",
    "# The parameters of the LSTM that keeps the model state.\n",
    "RNN_SIZE = 32\n",
    "RNN_PROJ = 32\n",
    "\n",
    "# Our training data follows the \"interpolated.csv\" format from Ross Wightman's scripts.\n",
    "CSV_HEADER = \"index,timestamp,width,height,frame_id,filename,angle,torque,speed,lat,long,alt\".split(\",\")\n",
    "OUTPUTS = CSV_HEADER[-6:-3] # angle,torque,speed\n",
    "OUTPUT_DIM = len(OUTPUTS) # predict all features: steering angle, torque and vehicle speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/output format\n",
    "Our data is presented as a long sequence of observations (several concatenated rosbags). We need to chunk it into a number of batches: for this, we will create BATCH_SIZE cursors. Let their starting points be uniformly spaced in our long sequence. We will advance them by SEQ_LEN at each step, creating a BATCH_SIZE x SEQ_LEN matrix of training examples. Boundary effects when one rosbag ends and the next starts are simply ignored.  \n",
    "\n",
    "(Actually, LEFT_CONTEXT frames are also added to the left of the input sequence; see code below for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, sequence, seq_len, batch_size):\n",
    "        self.sequence = sequence\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # (chunk_size - seq_len) is the number of batches in one epoch\n",
    "        chunk_size = 1 + (len(sequence) - 1) // batch_size\n",
    "        self.indices = [(i*chunk_size) % len(sequence) for i in range(batch_size)]\n",
    "        \n",
    "    def next(self):\n",
    "        \"\"\"A simple example:\n",
    "        - len(sequence) = 100\n",
    "        - batch_size = 5\n",
    "        - chunk_size = 1 + (100 - 1) /5 = 20\n",
    "        - indices = [0, 20, 40, 60, 80]\n",
    "        - seq_len = 4\n",
    "        each batch is [[0-4], [20-24], [40-44], [60-64], [80-84]] (ignore left_context)\"\"\"\n",
    "        while True:\n",
    "            output = []\n",
    "            for i in range(self.batch_size):\n",
    "                idx = self.indices[i]\n",
    "                left_pad = self.sequence[idx - LEFT_CONTEXT:idx]\n",
    "                if len(left_pad) < LEFT_CONTEXT:\n",
    "                    # duplicate inputs for left context if at the beginning\n",
    "                    left_pad = [self.sequence[0]] * (LEFT_CONTEXT - len(left_pad)) + left_pad\n",
    "                assert len(left_pad) == LEFT_CONTEXT\n",
    "                leftover = len(self.sequence) - idx\n",
    "                if leftover >= self.seq_len:\n",
    "                    result = self.sequence[idx:idx + self.seq_len]\n",
    "                else:\n",
    "                    # append inputs from beginning if we reach the end of the datasets\n",
    "                    result = self.sequence[idx:] + self.sequence[:self.seq_len - leftover]\n",
    "                assert len(result) == self.seq_len\n",
    "                \n",
    "                # modify indices so next time when we call next(), we will get another batch data starting from the \n",
    "                # end of last batch. This makes sure each batch has no overlap for the sequence\n",
    "                self.indices[i] = (idx + self.seq_len) % len(self.sequence)\n",
    "                \n",
    "                images, targets = zip(*result)\n",
    "                images_left_pad, _ = zip(*left_pad)\n",
    "                output.append((np.stack(images_left_pad + images), np.stack(targets)))\n",
    "                \n",
    "            output = zip(*output)\n",
    "            output[0] = np.stack(output[0]) # batch_size x (LEFT_CONTEXT + seq_len)\n",
    "            output[1] = np.stack(output[1]) # batch_size x seq_len x OUTPUT_DIM\n",
    "            return output\n",
    "        \n",
    "def read_csv(filename, prefix, nrows=None):\n",
    "    \"\"\"Helper function to read csv file\n",
    "    - prefix: the prefix path to be inserted in front of the image file name\n",
    "    - nrows: number of rows of file to read. Useful for read toy test data set\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        f.readline() # skip header row in csv file\n",
    "        lines = [ln.strip().split(\",\")[-7:-3] for ln in f.readlines()][:nrows]\n",
    "        lines = map(lambda x: (os.path.join(prefix, x[0]), np.float32(x[1:])), lines) # imagefile, outputs\n",
    "        return lines\n",
    "\n",
    "def process_csv(filename, prefix, nrows=None, val=5):\n",
    "    sum_f = np.float128([0.0] * OUTPUT_DIM)\n",
    "    sum_sq_f = np.float128([0.0] * OUTPUT_DIM)\n",
    "    lines = read_csv(filename, prefix, nrows=nrows)\n",
    "    # leave val% for validation\n",
    "    train_seq = []\n",
    "    valid_seq = []\n",
    "    cnt = 0\n",
    "    for ln in lines:\n",
    "        # only use images from the center camera\n",
    "        if 'center' not in ln[0]:\n",
    "            continue\n",
    "        if cnt < SEQ_LEN * BATCH_SIZE * (100 - val): \n",
    "            train_seq.append(ln)\n",
    "            sum_f += ln[1]\n",
    "            sum_sq_f += ln[1] * ln[1]\n",
    "        else:\n",
    "            valid_seq.append(ln)\n",
    "        cnt += 1\n",
    "        cnt %= SEQ_LEN * BATCH_SIZE * 100\n",
    "    mean = sum_f / len(train_seq)\n",
    "    var = sum_sq_f / len(train_seq) - mean * mean\n",
    "    std = np.sqrt(var)\n",
    "    print(len(train_seq), len(valid_seq))\n",
    "    print(mean, std) # we will need these statistics to normalize the outputs (and ground truth inputs)\n",
    "    return (train_seq, valid_seq), (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800 200\n",
      "[-0.0058325905 -0.082154441  23.879963] [ 0.068750301  0.52096972  2.5421321]\n"
     ]
    }
   ],
   "source": [
    "(train_seq, valid_seq), (mean, std) = process_csv(filename=\"SelfDrivingData/export_ch2_002/interpolated.csv\", \n",
    "                                                  prefix=\"SelfDrivingData/export_ch2_002\", nrows=12000,\n",
    "                                                  val=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key tricks\n",
    "Now we are ready to build the model. In the next cell we will define the vision module and the recurrent stateful cell.  \n",
    "\n",
    "The vision module takes a tensor of shape [BATCH_SIZE, LEFT_CONTEXT + SEQ_LEN, HEIGHT, WIDTH, CHANNELS] and outputs a tensor of shape [BATCH_SIZE, SEQ_LEN, 128]. The entire LEFT_CONTEXT is eaten by the 3D convolutions. Well-known tricks like residual connections and layer normalization are used to improve the convergence of the vision module. Dropout between each pair of layers serves as a regularizer.  \n",
    "\n",
    "We also need to define our own recurrent cell because we need to train our model jointly in two conditions: when it uses ground truth history and when it uses its own past predictions as the context for the future predictions.  \n",
    "\n",
    "In addition, we define two helper functions: a layer normalizer with trainable gain/offset and a gradient-clipping optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_norm = lambda x: tf.contrib.layers.layer_norm(inputs=x, center=True, scale=True, \n",
    "                                                    activation_fn=None, trainable=True)\n",
    "\n",
    "def get_optimizer(loss, lrate):\n",
    "    \"\"\"Adam optimizer with global norm gradients clipping.\"\"\"\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lrate)\n",
    "    gradvars = optimizer.compute_gradients(loss)\n",
    "    gradients, v = list(zip(*gradvars))\n",
    "    print([x.name for x in v])\n",
    "    \n",
    "    # clip gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 15.0)\n",
    "    return optimizer.apply_gradients(list(zip(gradients, v)))\n",
    "\n",
    "def apply_vision_simple(image, keep_prob, batch_size, seq_len, scope=None, reuse=None):\n",
    "    \n",
    "    # reshape sequence of images to have proper shape fed into 3D-conv\n",
    "    video = tf.reshape(image, shape=[batch_size, LEFT_CONTEXT + seq_len, HEIGHT, WIDTH, CHANNELS])\n",
    "    \n",
    "    with tf.variable_scope(scope, 'Vision', [image], reuse=reuse):\n",
    "        \n",
    "        # 3-D conv layers and auxiliary output\n",
    "        \n",
    "        # conv-1\n",
    "        # in sequence dimension, 3-D conv eats 3-1=2 images, 5-2=3 left context left\n",
    "        net = slim.convolution(video, num_outputs=64, kernel_size=[3,12,12], stride=[1,6,6], padding=\"VALID\")\n",
    "        net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "        # net has 5 dimensions: batch size, sequence, height, width and channels\n",
    "        aux1 = slim.fully_connected(tf.reshape(net[:, -seq_len:, :, :, :], [batch_size, seq_len, -1]), \n",
    "                                    128, activation_fn=None)\n",
    "        \n",
    "        # conv-2\n",
    "        # in sequence dimension, 3-D conv eats 2-1=1 image, 3-1=2 left context left\n",
    "        net = slim.convolution(net, num_outputs=64, kernel_size=[2,5,5], stride=[1,2,2], padding=\"VALID\")\n",
    "        net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "        aux2 = slim.fully_connected(tf.reshape(net[:, -seq_len:, :, :, :], [batch_size, seq_len, -1]), \n",
    "                                    128, activation_fn=None)\n",
    "        \n",
    "        # conv-3\n",
    "        # in sequence dimension, 3-D conv eats 2-1=1 image, 2-1=1 left context left\n",
    "        net = slim.convolution(net, num_outputs=64, kernel_size=[2,5,5], stride=[1,1,1], padding=\"VALID\")\n",
    "        net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "        aux3 = slim.fully_connected(tf.reshape(net[:, -seq_len:, :, :, :], [batch_size, seq_len, -1]), \n",
    "                                    128, activation_fn=None)\n",
    "        \n",
    "        # conv-4\n",
    "        # in sequence dimension, 3-D conv eats 2-1=1 image, 1-1=0 left context left\n",
    "        net = slim.convolution(net, num_outputs=64, kernel_size=[2,5,5], stride=[1,1,1], padding=\"VALID\")\n",
    "        net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "        \n",
    "        # at this point the tensor 'net' is of shape batch_size * seq_len * ... (all left context has been eaten)\n",
    "        aux4 = slim.fully_connected(tf.reshape(net, [batch_size, seq_len, -1]), 128, activation_fn=None)\n",
    "        \n",
    "        # fully connected layers\n",
    "        net = slim.fully_connected(tf.reshape(net, [batch_size, seq_len, -1]), 1024, activation_fn=tf.nn.relu)\n",
    "        net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "        \n",
    "        net = slim.fully_connected(net, 512, activation_fn=tf.nn.relu)\n",
    "        net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "        \n",
    "        net = slim.fully_connected(net, 256, activation_fn=tf.nn.relu)\n",
    "        net = tf.nn.dropout(x=net, keep_prob=keep_prob)\n",
    "        \n",
    "        # final output layer\n",
    "        net = slim.fully_connected(net, 128, activation_fn=None)\n",
    "        \n",
    "        # aux[1-4] are residual connections (shortcuts) (structure like ResNet)\n",
    "        return layer_norm(tf.nn.elu(net + aux1 + aux2 + aux3 + aux4)) \n",
    "\n",
    "# self-defined RNN Cell, only deal with one time step data [batch_size, features]\n",
    "# used as inputs to tf.nn.dynamic_rnn later\n",
    "# and tf.nn.dynamic_rnn will deal with the sequence \n",
    "class SamplingRNNCell(tf.contrib.rnn.BasicRNNCell):\n",
    "    \"\"\"Simple sampling RNN cell.\"\"\"\n",
    "\n",
    "    def __init__(self, num_outputs, use_ground_truth, internal_cell):\n",
    "        \"\"\"\n",
    "        if use_ground_truth then don't sample\n",
    "        \"\"\"\n",
    "        self._num_outputs = num_outputs\n",
    "        self._use_ground_truth = use_ground_truth # boolean\n",
    "        self._internal_cell = internal_cell # may be LSTM or GRU or anything\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_outputs, self._internal_cell.state_size # previous output and bottleneck state\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_outputs # steering angle, torque, vehicle speed\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        (visual_feats, current_ground_truth) = inputs\n",
    "        prev_output, prev_state_internal = state\n",
    "        context = tf.concat([prev_output, visual_feats], 1)\n",
    "        # here the internal cell (e.g. LSTM) is called\n",
    "        new_output_internal, new_state_internal = internal_cell(context, prev_state_internal) \n",
    "        \n",
    "        # autoregressive part?\n",
    "        new_output = tf.contrib.layers.fully_connected(\n",
    "            inputs=tf.concat([new_output_internal, prev_output, visual_feats], 1),\n",
    "            num_outputs=self._num_outputs,\n",
    "            activation_fn=None,\n",
    "            scope=\"OutputProjection\")\n",
    "        # if self._use_ground_truth == True, \n",
    "        # we pass the ground truth as the state; otherwise, we use the model's predictions\n",
    "        return new_output, (current_ground_truth if self._use_ground_truth else new_output, new_state_internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Let's build the main graph. Code is mostly self-explanatory.  \n",
    "\n",
    "A few comments:  \n",
    "\n",
    "1) PNG images were used as the input only because this was the format for round1 testset. In practice, raw images should be fed directly from the rosbags.  \n",
    "\n",
    "2) We define get_initial_state and deep_copy_initial_state functions to be able to preserve the state of our recurrent net between batches. (The second batch is right after the first batch in time series, so the final state of the first batch should be the initial state for the second batch.) The backpropagation is still truncated by SEQ_LEN.  \n",
    "\n",
    "3) The loss is composed of two components. The first is the MSE of the steering angle prediction in the autoregressive setting -- that is exactly what interests us in the test time. The second components, weighted by the term aux_cost_weight, is the sum of MSEs for all outputs both in autoregressive and ground truth settings.  \n",
    "\n",
    "Note: if the saver definition doesn't work for you please make sure you are using tensorflow 0.12rc0 or newer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Vision/Conv/weights:0', u'Vision/Conv/biases:0', u'Vision/fully_connected/weights:0', u'Vision/fully_connected/biases:0', u'Vision/Conv_1/weights:0', u'Vision/Conv_1/biases:0', u'Vision/fully_connected_1/weights:0', u'Vision/fully_connected_1/biases:0', u'Vision/Conv_2/weights:0', u'Vision/Conv_2/biases:0', u'Vision/fully_connected_2/weights:0', u'Vision/fully_connected_2/biases:0', u'Vision/Conv_3/weights:0', u'Vision/Conv_3/biases:0', u'Vision/fully_connected_3/weights:0', u'Vision/fully_connected_3/biases:0', u'Vision/fully_connected_4/weights:0', u'Vision/fully_connected_4/biases:0', u'Vision/fully_connected_5/weights:0', u'Vision/fully_connected_5/biases:0', u'Vision/fully_connected_6/weights:0', u'Vision/fully_connected_6/biases:0', u'Vision/fully_connected_7/weights:0', u'Vision/fully_connected_7/biases:0', u'Vision/LayerNorm/beta:0', u'Vision/LayerNorm/gamma:0', u'controller_initial_state_0:0', u'controller_initial_state_1:0', u'controller_initial_state_2:0', u'predictor/rnn/lstm_cell/weights:0', u'predictor/rnn/lstm_cell/biases:0', u'predictor/rnn/lstm_cell/projection/weights:0', u'predictor/rnn/OutputProjection/weights:0', u'predictor/rnn/OutputProjection/biases:0']\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # inputs\n",
    "    learning_rate = tf.placeholder_with_default(input=1e-4, shape=())\n",
    "    keep_prob = tf.placeholder_with_default(input=1.0, shape=())\n",
    "    aux_cost_weight = tf.placeholder_with_default(input=0.1, shape=())\n",
    "    \n",
    "    # pathes to jpeg files from the central camera\n",
    "    inputs = tf.placeholder(shape=(BATCH_SIZE,LEFT_CONTEXT+SEQ_LEN), dtype=tf.string) \n",
    "    # batch_size * seq_len * OUTPUT_DIM\n",
    "    targets = tf.placeholder(shape=(BATCH_SIZE,SEQ_LEN,OUTPUT_DIM), dtype=tf.float32) \n",
    "    \n",
    "    # mean and std are calculated above when reading data\n",
    "    targets_normalized = (targets - mean) / std # (batch_size, seq_len, output_dim)\n",
    "    \n",
    "    # read matrix of images based on matrix of image file paths\n",
    "    # tf.image.decode_jpeg will return a 3-D array of shape [height, width, channels]\n",
    "    input_images = tf.stack([tf.image.decode_jpeg(tf.read_file(x)) # a list of 3-D array get from decode_jpeg\n",
    "                            for x in tf.unstack(tf.reshape(inputs, shape=[(LEFT_CONTEXT+SEQ_LEN) * BATCH_SIZE]))])\n",
    "    \n",
    "    # could add data augmentation or flipping here\n",
    "    \n",
    "    # normalize images (0 to 255 -> -1 to 1)\n",
    "    input_images = -1.0 + 2.0 * tf.cast(input_images, tf.float32) / 255.0\n",
    "    \n",
    "    # Updates the shape of this tensor (not changing the shape, different from tf.reshape)\n",
    "    input_images.set_shape([(LEFT_CONTEXT+SEQ_LEN) * BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])\n",
    "    \n",
    "    # output has shape [batch_size, seq_len, 128]\n",
    "    visual_conditions_reshaped = apply_vision_simple(image=input_images, keep_prob=keep_prob, \n",
    "                                                     batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n",
    "    \n",
    "    visual_conditions = tf.reshape(visual_conditions_reshaped, [BATCH_SIZE, SEQ_LEN, -1])\n",
    "    visual_conditions = tf.nn.dropout(x=visual_conditions, keep_prob=keep_prob)\n",
    "    \n",
    "    # prepare inputs to RNN based on output from 3-D conv layers\n",
    "    # - visual_conditions: (batch_size, seq_len, 128)\n",
    "    # - targets_normalized: (BATCH_SIZE, SEQ_LEN, OUTPUT_DIM)\n",
    "    rnn_inputs_with_ground_truth = (visual_conditions, targets_normalized)\n",
    "    rnn_inputs_autoregressive = (visual_conditions, \n",
    "                                 tf.zeros(shape=(BATCH_SIZE, SEQ_LEN, OUTPUT_DIM), dtype=tf.float32))\n",
    "    \n",
    "    # num_units: size of H, by default, LSTM is simaply a way to get Ht given Ht-1 and Xt\n",
    "    # num_proj: size of Yt, a linear projection after Ht\n",
    "    internal_cell = tf.contrib.rnn.LSTMCell(num_units=RNN_SIZE, num_proj=RNN_PROJ)\n",
    "    cell_with_ground_truth = SamplingRNNCell(num_outputs=OUTPUT_DIM, use_ground_truth=True, \n",
    "                                             internal_cell=internal_cell)\n",
    "    cell_autoregressive = SamplingRNNCell(num_outputs=OUTPUT_DIM, use_ground_truth=False, \n",
    "                                          internal_cell=internal_cell)\n",
    "    \n",
    "    def get_initial_state(complex_state_tuple_sizes):\n",
    "        \n",
    "        # complex_state_tuple_sizes: (_num_outputs, (c_state_size, m_state_size))\n",
    "        flat_sizes = nest.flatten(complex_state_tuple_sizes)\n",
    "        init_state_flat = [tf.tile( # tf.tile: repeat inputs in each dimension by i times\n",
    "            multiples=[BATCH_SIZE, 1], \n",
    "            input=tf.get_variable(\"controller_initial_state_%d\" % i, initializer=tf.zeros_initializer, \n",
    "                                  shape=([1, s]), dtype=tf.float32))\n",
    "         for i,s in enumerate(flat_sizes)]\n",
    "        \n",
    "        # each batch element can have a different initial state\n",
    "        # pack flat state back to a nested tuple: (new_output, (c_state, m_state))\n",
    "        # - new_output: (batch_size, _num_outputs)\n",
    "        # - c_state: (batch_size, c_state_size)\n",
    "        # - m_state: (batch_size, m_state_size)\n",
    "        init_state = nest.pack_sequence_as(complex_state_tuple_sizes, init_state_flat)\n",
    "        return init_state\n",
    "    \n",
    "    def deep_copy_initial_state(complex_state_tuple):\n",
    "        flat_state = nest.flatten(complex_state_tuple)\n",
    "        flat_copy = [tf.identity(s) for s in flat_state] # tf.identity copies shape and content of a tensor\n",
    "        deep_copy = nest.pack_sequence_as(complex_state_tuple, flat_copy)\n",
    "        return deep_copy\n",
    "    \n",
    "    controller_initial_state_variables = get_initial_state(cell_autoregressive.state_size)\n",
    "    \n",
    "    # initial_state for autoregressive case\n",
    "    controller_initial_state_autoregressive = deep_copy_initial_state(controller_initial_state_variables)\n",
    "    # initial_state for groud truth case\n",
    "    controller_initial_state_gt = deep_copy_initial_state(controller_initial_state_variables)\n",
    "\n",
    "    with tf.variable_scope(\"predictor\"):\n",
    "        # - out_gt: (batch_size, seq_len, num_outputs)\n",
    "        out_gt, controller_final_state_gt = \\\n",
    "        tf.nn.dynamic_rnn(cell=cell_with_ground_truth, \n",
    "                          inputs=rnn_inputs_with_ground_truth, \n",
    "                          sequence_length=[SEQ_LEN]*BATCH_SIZE, initial_state=controller_initial_state_gt, \n",
    "                          dtype=tf.float32, swap_memory=True, time_major=False)\n",
    "        \n",
    "    with tf.variable_scope(\"predictor\", reuse=True):\n",
    "        # reuse the variable above\n",
    "        out_autoregressive, controller_final_state_autoregressive = \\\n",
    "        tf.nn.dynamic_rnn(cell=cell_autoregressive, \n",
    "                          inputs=rnn_inputs_autoregressive, \n",
    "                          sequence_length=[SEQ_LEN]*BATCH_SIZE, \n",
    "                          initial_state=controller_initial_state_autoregressive, \n",
    "                          dtype=tf.float32, swap_memory=True, time_major=False)\n",
    "    \n",
    "    # mse for all outputs (angle, torque, speed) across 3 dimensions (batch_size, seq_len, num_outputs)\n",
    "    mse_gt = tf.reduce_mean(tf.squared_difference(out_gt, targets_normalized))\n",
    "    mse_autoregressive = tf.reduce_mean(tf.squared_difference(out_autoregressive, targets_normalized))\n",
    "    \n",
    "    # mse for angle, across 2 dimensions (batch_size, seq_len)\n",
    "    mse_autoregressive_steering = tf.reduce_mean(tf.squared_difference(out_autoregressive[:, :, 0], \n",
    "                                                                       targets_normalized[:, :, 0]))\n",
    "    \n",
    "    steering_predictions = (out_autoregressive[:, :, 0] * std[0]) + mean[0]\n",
    "    \n",
    "    # final scalar loss to be minimized\n",
    "    total_loss = mse_autoregressive_steering + aux_cost_weight * (mse_gt + mse_autoregressive)\n",
    "    \n",
    "    optimizer = get_optimizer(total_loss, learning_rate)\n",
    "    \n",
    "    # summary\n",
    "    tf.summary.scalar(\"MAIN_TRAIN_METRIC_rmse_autoregressive_steering\", tf.sqrt(mse_autoregressive_steering))\n",
    "    tf.summary.scalar(\"rmse_gt\", tf.sqrt(mse_gt))\n",
    "    tf.summary.scalar(\"rmse_autoregressive\", tf.sqrt(mse_autoregressive))\n",
    "    \n",
    "    summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('v3/train_summary', graph=graph)\n",
    "    valid_writer = tf.summary.FileWriter('v3/valid_summary', graph=graph)\n",
    "    saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "At this point we can start the training procedure.  \n",
    "\n",
    "We will perform optimization for 100 epochs, doing validation after each epoch. We will keep the model's version that obtains the best performance in terms of the primary loss (autoregressive steering MSE) on the validation set. An aggressive regularization is used (keep_prob=0.25 for dropout), and the validation loss is highly non-monotonical.  \n",
    "\n",
    "For each version of the model that beats the previous best validation score we will overwrite the checkpoint file and obtain predictions for the challenge test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Starting epoch 0\n",
      "Validation:\n",
      " 1 / 5 2.0044540638\n",
      " 2 / 5 2.00480503799\n",
      " 3 / 5 2.02282143816\n",
      " 4 / 5 2.0305336496\n",
      " 5 / 5 2.03089749139\n",
      "\n",
      "Training\n"
     ]
    }
   ],
   "source": [
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "\n",
    "checkpoint_dir = os.getcwd() + \"/v3\"\n",
    "\n",
    "global_train_step = 0\n",
    "global_valid_step = 0\n",
    "\n",
    "KEEP_PROB_TRAIN = 0.25\n",
    "\n",
    "def do_epoch(session, sequences, mode):\n",
    "    global global_train_step, global_valid_step\n",
    "    \n",
    "    test_predictions = {}\n",
    "    valid_predictions = {}\n",
    "    batch_generator = BatchGenerator(sequence=sequences, seq_len=SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # total steps in one epoch, chunk_size / seq_len\n",
    "    total_num_steps = 1 + (batch_generator.indices[1] - 1) // SEQ_LEN\n",
    "    controller_final_state_gt_cur, controller_final_state_autoregressive_cur = None, None\n",
    "    acc_loss = np.float128(0.0)\n",
    "    \n",
    "    for step in range(total_num_steps):\n",
    "        feed_inputs, feed_targets = batch_generator.next()\n",
    "        feed_dict = {inputs : feed_inputs, targets : feed_targets}\n",
    "        \n",
    "        # feed in final state in last batch to the next batch as initial state\n",
    "        if controller_final_state_autoregressive_cur is not None:\n",
    "            feed_dict.update({controller_initial_state_autoregressive : controller_final_state_autoregressive_cur})\n",
    "        if controller_final_state_gt_cur is not None:\n",
    "            # original code: feed_dict.update({controller_final_state_gt : controller_final_state_gt_cur})\n",
    "            # which might be wrong?\n",
    "            feed_dict.update({controller_initial_state_gt : controller_final_state_gt_cur})\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            feed_dict.update({keep_prob : KEEP_PROB_TRAIN})\n",
    "            \n",
    "            # only use groud truth in autoregressive during training\n",
    "            summary, _, loss, controller_final_state_gt_cur, controller_final_state_autoregressive_cur = \\\n",
    "                session.run([summaries, optimizer, mse_autoregressive_steering, controller_final_state_gt, \n",
    "                             controller_final_state_autoregressive], feed_dict = feed_dict)\n",
    "            \n",
    "            train_writer.add_summary(summary, global_train_step)\n",
    "            global_train_step += 1\n",
    "        \n",
    "        elif mode == \"valid\":\n",
    "            model_predictions, summary, loss, controller_final_state_autoregressive_cur = \\\n",
    "                session.run([steering_predictions, summaries, mse_autoregressive_steering, \n",
    "                             controller_final_state_autoregressive],feed_dict = feed_dict)\n",
    "            \n",
    "            valid_writer.add_summary(summary, global_valid_step)\n",
    "            global_valid_step += 1  \n",
    "            \n",
    "            # record validation predictions and error\n",
    "            feed_inputs = feed_inputs[:, LEFT_CONTEXT:].flatten()\n",
    "            steering_targets = feed_targets[:, :, 0].flatten() # (batch_size * seq_len,)\n",
    "            model_predictions = model_predictions.flatten()\n",
    "            \n",
    "            # record true target, prediction, error\n",
    "            # stats: (3, batch_size * seq_len)\n",
    "            stats = np.stack([steering_targets, model_predictions, (steering_targets - model_predictions)**2])\n",
    "            for i, img in enumerate(feed_inputs):\n",
    "                valid_predictions[img] = stats[:, i]\n",
    "                \n",
    "        elif mode == \"test\":\n",
    "            model_predictions, controller_final_state_autoregressive_cur = \\\n",
    "                session.run([steering_predictions, controller_final_state_autoregressive],\n",
    "                           feed_dict = feed_dict) \n",
    "            \n",
    "            # record test predictions and error\n",
    "            feed_inputs = feed_inputs[:, LEFT_CONTEXT:].flatten()\n",
    "            model_predictions = model_predictions.flatten()\n",
    "            for i, img in enumerate(feed_inputs):\n",
    "                test_predictions[img] = model_predictions[i]\n",
    "        \n",
    "        # print average accumulative loss (mse_autoregressive_steering)\n",
    "        if mode != \"test\":\n",
    "            acc_loss += loss\n",
    "            print('\\r', step + 1, \"/\", total_num_steps, np.sqrt(acc_loss / (step+1)),)\n",
    "    print()\n",
    "    return (np.sqrt(acc_loss / total_num_steps), valid_predictions) if mode != \"test\" else (None, test_predictions)\n",
    "    \n",
    "\n",
    "NUM_EPOCHS=100\n",
    "\n",
    "best_validation_score = None\n",
    "# add 'gpu_options=gpu_options' to tf.ConfigProto() if using GPU\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto()) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if ckpt:\n",
    "        print(\"Restoring from\", ckpt)\n",
    "        saver.restore(sess=session, save_path=ckpt)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"Starting epoch %d\" % epoch)\n",
    "        \n",
    "        # validate before each epoch\n",
    "        print(\"Validation:\")\n",
    "        \n",
    "        # valid_score is the average accumulative loss (mse_autoregressive_steering) returned by do_epoch\n",
    "        valid_score, valid_predictions = do_epoch(session=session, sequences=valid_seq, mode=\"valid\")\n",
    "        if best_validation_score is None: \n",
    "            best_validation_score = valid_score\n",
    "        \n",
    "        # update best valid score and save best model so far\n",
    "        if valid_score < best_validation_score:\n",
    "            saver.save(session, 'v3/checkpoint-sdc-ch2')\n",
    "            best_validation_score = valid_score\n",
    "            print('\\r', \"SAVED at epoch %d\" % epoch,)\n",
    "            \n",
    "            # validation: write image file name and corresponding stats (prediction, error) to a file\n",
    "            with open(\"v3/valid-predictions-epoch%d\" % epoch, \"w\") as out:\n",
    "                result = np.float128(0.0)\n",
    "                for img_id, stats in valid_predictions.items():\n",
    "                    print(img_id, stats, file=out)\n",
    "                    # accumulate SE, different from mse_autoregressive_steering\n",
    "                    result += stats[-1]\n",
    "            print(\"Validation unnormalized RMSE:\", np.sqrt(result / len(valid_predictions)))\n",
    "            \n",
    "#             # test: write image file name and corresponding stats (prediction, error) to a file\n",
    "#             with open(\"v3/test-predictions-epoch%d\" % epoch, \"w\") as out:\n",
    "#                 _, test_predictions = do_epoch(session=session, sequences=test_seq, mode=\"test\")\n",
    "#                 # write header\n",
    "#                 print(\"frame_id,steering_angle\", file=out)\n",
    "#                 for img_id, pred in test_predictions.items():\n",
    "#                     img_id = img_id.replace(\"challenge_2/Test-final/center/\", \"\")\n",
    "#                     print(\"%s,%f\" % (img, pred), file=out)\n",
    "        \n",
    "        # continue training if not reach the number of epochs\n",
    "        if epoch != NUM_EPOCHS - 1:\n",
    "            print(\"Training\")\n",
    "            do_epoch(session=session, sequences=train_seq, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically that's it.  \n",
    "\n",
    "The model can be further fine-tuned for the challenge purposes by subsetting the training set and setting the aux_cost_weight to zero. It improves the result slightly, but the improvement is marginal (doesn't affect the challenge ranking). For real-life usage it would be probably harmful because of the risk of overfitting to the dev- or even testset.  \n",
    "\n",
    "Of course, speaking of realistic models, we don't need to constrain our input only to the central camera -- other cameras and sensors can dramatically improve the performance. Also it is useful to think of a realistic delay for the target sequence to make an actual non-zero-latency control possible.  \n",
    "\n",
    "If something in this writeup is unclear, please write me a e-mail so that I can add the necessary comments/clarifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
